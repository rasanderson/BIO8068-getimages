---
title: "Using wildlife images from Citizen Science databases"
subtitle: "BIO8068 Data visualisation in Ecology"
output:
  html_document: default
  word_document:
    reference_docx: template.docx
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rinat)
library(sf)
library(keras)

source("download_images.R")
gb_ll <- readRDS("gb_simple.RDS")
```

# Introduction
You may have noticed from looking at some of the Citizen Science databases we have explored earlier that records often have photographs associated with them. Go back and look at the National Biodiversity Network (NBN Atlas), Global Biodiversity Information Facility (GBIF) and xeno-canto birdsong databases and you'll see that some, but not all, records have photographs. The quality of photographs is extremely variable, and it is usually the more recently submitted records that are likely to have photographs.

The international iNaturalist website <https://www.inaturalist.org/> and UK-based iRecord website <https://www.brc.ac.uk/irecord/> both provide access to high quality Citizen Science datasets. Both databases have mobile phone apps through which users can submit records, and as a result a high proportion of records include photographs. The UK iRecord site is more restricted in that it does not allow general downloading of images (although they can be viewed on the website).

# The "data bottleneck" in deep learning
You have already created a simple Convolutional Neural Network (CNN) to identify different species, and this required you to have lots of photographs that had already been labelled, that you could then put into your training or validation datasets to create the models. One problem is obtaining sufficient data for machine learning models that has already been labelled correctly. Often this process has to be done by hand, and is time consuming.

# Aims and objectives
In this workshop you'll learn how to quickly obtain images from the iNaturalist Citizen Science database that can then be used in deep learning models. It will give you an understanding of the problems of data quality that can arise, as well as better technical insights. We shall pick several contrasting butterfly species as an example to work with.

# Bulk download brimstone butterfly images
Begin by loading the `rinat` package with `library(rinat)` which provides useful utility functions such as `get_inat_obs` that you have already used to retrieve records. You should also load the `sf` package with `library(sf)` as iNaturalist is an international biodiversity website, and for the purposes of this practical exercise we are going to restrict our search to Great Britain. Actually downloading the images takes a bit of R coding, and requires some additional packages to be installed. I have prepared a script, which will setup the additional packages, in particular `RCurl` for you, and also create a function called `download_images()` which does the hard work of downloading and renaming images. Finally, we shall define a bounding box for Great Britain based on the `gb_simple.RDS` file you have already used.

```{r, eval=FALSE}
library(rinat)
library(sf)

# Both download_images.R and gb_simple.RDS available on Canvas
source("download_images.R") 
gb_ll <- readRDS("gb_simple.RDS")
```

Once you have setup your working environment, we can search for records with the `get_inat_obs()` function. Look at the help for this function and you will see that it is quite flexible, in that you can specify a particular month or year. We will restrict our search to a maximum of 600 records, and specify they must be of "research" quality. Again, I must stress that this is a very small dataset, and usually you would have far more to work with. After downloading, we will split each set of images into 3 groups:

* training dataset: this is used by the machine learning model to develop its weights
* validation dataset: during the training process, the outputs are continually compared with the validation dataset, and the weights updated to improve the fit
* test dataset: completely independent, to check model accuracy

Typical splits of the data vary, common ones being 70% train, 15% validation, 15% test, and also 80:10:10 and 60:20:20 are also common. To keep the numbers simple, we'll use 400 images for training, 100 for validation, and 100 for testing. Admittedly this is 67:17:17 ratio which is not standard! I could ask you to download 1000 images (800:100:100) but suspect the download times might make you bored.

After you have downloaded the images, you'll need to put them into two sets of folders:

* An `images` folder, with subfolders for each species. Images will be downloaded into this folder from iNaturalist automatically split into the training and validation sets from here
* A `test` folder, with subfolders for each species. You will have to move the last 100 images for each species into here manually.

We'll begin by searching for records of the brimstone butterfly, _Gonepteryx rhamni_ <https://butterfly-conservation.org/butterflies/brimstone> You could do a general searh for "brimstone" but this would also return records for the completely different brimstone moth _Opisthograptis luteolata_. They are both yellow in colour, but common names can be misleading, so stick to the Latin names:

```{r, eval=FALSE}
brimstone_recs <-  get_inat_obs(taxon_name  = "Gonepteryx rhamni",
                               bounds = gb_ll,
                               quality = "research",
                               # month=6,   # Month can be set.
                               # year=2018, # Year can be set.
                               maxresults = 600)
```

If you look at the records, you'll see that most of them have a URL to the actual image. If you copy a URL into your web browser you can have a look at one of them. If you wanted, you could `filter` your results, remember to `library(dplyr)`, at this stage for example to remove any records tagged as larvae rather than adults. Unfortunately, in general this sort of information is missing for most records, so is hard to automate. However, in the UK the larvae are most active May-July, so you could always `filter` out records for those months if you find too many caterpillar photographs.

Now use the `download_images()` function to retrieve the images. This has the following arguments:

* `spp_recs` The list of records from iNaturalist, possibly after a `filter` to improve quality (required)
* `spp_folder` The name of the folder into which the image files will be saved (required)
* `image_folder` By default this is a subfolder within your RStudio Project called `images` and will be created for you if it does not exist (optional)

Depending on the speed of your internet connection it may take 20 to 30 minutes or more to download a large number of images. A popup screen will indicate progress, so you will have an understanding of how long it is likely to take:

```{r, eval=FALSE}
download_images(spp_recs = brimstone_recs, spp_folder = "brimstone")
```

As the files download, you'll notice that they all have exactly the same name of `medium.jpg` so the `download_images()` function renames them `spp_1.jpg`, `spp_2.jpg`, `spp_3.jpg` etc. to avoid over-writing. They will be stored in a subfolder called `brimstone` within your `images` folder. Each individual file is fairly small, but collectively your downloaded images will probably be over 50 Mb. Go to the `brimstone` subfolder in File Explorer (Windows) or Finder (Mac) and look at some of them. Most of mine are of adult butterflies, but there are a small number of images with larvae, or the butterfly is difficult to see. There are also quite a lot of images where the main object in the photograph is actually a flower from which the butterflies are collecting nectar, typically purple or pink-coloured flowers. Be alert that if you train a model on other species of butterflies that have similar feeding preferences the model may learn to identify the flowers, rather than the butterflies!

# Bulk download holly blue and orange tip butterfly records and images
Now we'll repeat the process for two other species of common butterflies in a similar way, the holly blue <https://butterfly-conservation.org/butterflies/holly-blue> and the orange tip <https://butterfly-conservation.org/butterflies/orange-tip>:

```{r, eval=FALSE}
# Holly blue; Celastrina argiolus
hollyblue_recs <-  get_inat_obs(taxon_name  = "Celastrina argiolus",
                               bounds = gb_ll,
                               quality = "research",
                               maxresults = 600)


# Orange tip; Anthocharis cardamines
orangetip_recs <-  get_inat_obs(taxon_name  = "Anthocharis cardamines",
                               bounds = gb_ll,
                               quality = "research",
                               maxresults = 600)
```

One thing to be alert to with both these species is that the males are much more brightly coloured than the females. The overwhelming majority of records of butterflies on iNaturalist do not unfortunately distinguish between male and female photographs, and this will decrease the accuracy of any deep learning models. After you have downloaded the records, again explore them using `View(hollyblue_recs)` or `View(orangetip_recs)`. Nearly all the records include a link to a photograph.

Next download the images, which will be stored in two subfolders, named accordingly, within your `images` folder. You may want to make a cup of tea or coffee whilst the downloads take place...

```{r, eval=FALSE}
download_images(spp_recs = hollyblue_recs, spp_folder = "hollyblue")
download_images(spp_recs = orangetip_recs, spp_folder = "orangetip")
```

If you look at the photographs in the `hollyblue` and `orangetip` subfolders, you'll notice that the imagery for the latter is not as well-defined. This is likely to give poorer performance in any deep-learning model.

## Put test images into separate folder
Ideally we would have several thousand images for each of our three species, indeed many deep learning approaches uses tens of thousands of images. However I did not want you to have to wait too long to download the images, or do the training, but keep this weakness in mind when you analyse the data. We will the first 500 for training and validation, with 80% (400 per species) for training, and 20% (100 per species) for validation. This 80:20 split can be done automatically by Keras.

However, we need to manually have some totally independent images (100 per species) for testing. We'll use the files `spp_501.jpg` to `spp_600.jpg` for each species for this. We can easily move them into a separate folder with the right structure:

```{r, eval=FALSE}
image_files_path <- "images" # path to folder with photos

# list of spp to model; these names must match folder names
animal_list <- dir(image_files_path) # Automatically pick up names
#animal_list <- c("brimstone", "hollyblue", "orangetip") # manual entry

# number of spp classes (i.e. 3 species in this example)
output_n <- length(animal_list)

# Create test, and species sub-folders
for(folder in 1:output_n){
  dir.create(paste("test", animal_list[folder], sep="/"), recursive=TRUE)
}

# Now copy over spp_501.jpg to spp_600.jpg using two loops, deleting the photos
# from the original images folder after the copy
for(folder in 1:output_n){
  for(image in 501:600){
    src_image  <- paste0("images/", animal_list[folder], "/spp_", image, ".jpg")
    dest_image <- paste0("test/"  , animal_list[folder], "/spp_", image, ".jpg")
    file.copy(src_image, dest_image)
    file.remove(src_image)
  }
}
```


## Train up your deep learning model
### Initial setup
Now you have downloaded your image data from iNaturalist and sorted it at random into training and validation sets, you can create a deep learning model using a convolutional neural network, similar to your earlier example. The following code is almost identical to the one you used earlier, with only some minor changes. Remember to load the `keras` package using `library(keras)` before you continue:

```{r}

# image size to scale down to (original images vary but about 400 x 500 px)
img_width <- 150
img_height <- 150
target_size <- c(img_width, img_height)

# Full-colour Red Green Blue = 3 channels
channels <- 3
```

Next rescale your images (255 is max colour hue) and define the proportion (20%) that will be used for validation

```{r}
# Rescale from 255 to between zero and 1
train_data_gen = image_data_generator(
  rescale = 1/255,
  validation_split = 0.2
)
```

### Reading all the images from a folder
The `flow_images_from_directory()` function batches-processes the photographs according to the `image_data_generator()` defined above. We call it twice, once to define the images for training, and once for the validation. To ensure reproducibility, so you get the same results each time you run it, I've included a random number generator `seed`, although note that you will get slightly different results to me. The `subset` option defines whether the images will be assigned to the training or validation sets.

**Note** You could manually put your images into separate training and validation sub-folders, and define separate `image_files_paths` and omit the `subset` option, but this takes longer to configure.

```{r}
# training images
train_image_array_gen <- flow_images_from_directory(image_files_path, 
                                                    train_data_gen,
                                                    target_size = target_size,
                                                    class_mode = "categorical",
                                                    classes = animal_list,
                                                    subset = "training",
                                                    seed = 42)

# validation images
valid_image_array_gen <- flow_images_from_directory(image_files_path, 
                                                    train_data_gen,
                                                    target_size = target_size,
                                                    class_mode = "categorical",
                                                    classes = animal_list,
                                                    subset = "validation",
                                                    seed = 42)

```

When you run the two commands above, it should confirm that it has correctly identified different sets of images for the model. Next, check that we seem to have the right number off classes and images:

```{r check generator}
# Check that things seem to have been read in OK
cat("Number of images per class:")
table(factor(train_image_array_gen$classes))
cat("Class labels vs index mapping")
train_image_array_gen$class_indices
```

```{r, echo=FALSE}
detach("package:keras", unload = TRUE)
detach("package:imager", unload = TRUE)
```

To look at one of your images you can display it via the `as.raster()` function (note, you may get a different image displayed):

```{r}
plot(as.raster(train_image_array_gen[[1]][[1]][8,,,]))
```

You might be puzzling about all the sets of double-square brackets etc. This is because the `train_image_array_gen` is actually quite complex, containing information in an R list structure, that will be passed to Python. If you check the size of the first element with `dim(train_image_array_gen[[1]][[1]])` you will see it returns `32 150 150 3`. This is because by default Keras sub-divides your images into "batches" of 32 images for ease of processing, although you can over-ride this. The 150 x 150 is the size of your rescaled image that you defined earlier. The 3 represents RGB for red, green and blue full-colour images.

### Define additional parameters and configure model
Next define some 'hyper-parameters' such as batch size for numbers of images flowing through the system with each 'epoch'. I'll define 

```{r final setup, eval=FALSE}
# number of training samples
train_samples <- train_image_array_gen$n
# number of validation samples
valid_samples <- valid_image_array_gen$n

# define batch size and number of epochs
batch_size <- 32 # Useful to define explicitly as we'll use it later
epochs <- 10     # How long to keep training going for
```

Now you define how your CNN is structured.

```{r define CNN structure, eval=FALSE}
# initialise model
model <- keras_model_sequential()

# add layers
model %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), input_shape = c(img_width, img_height, channels), activation = "relu") %>%

  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), activation = "relu") %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %>%
  layer_dense(100, activation = "relu") %>%
  layer_dropout(0.5) %>%
  
  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n, activation = "softmax") 
```


Remember to check the CNN structure before compiling and running it

```{r check CNN structure, eval=FALSE}
print(model)
```

Define the error terms and accuracy measures. Use `categorical_crossentropy` as we have more than two species:

```{r compile, eval=FALSE}
# Compile the model
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),
  metrics = "accuracy"
)
```

### Set the deep-learning model off (slow!)
Finally, train up the model. **This is slow**. Depending on your PC, it may take more than 30 minutes to complete. On my PC it takes about 10 minutes, but runs at 100% CPU and the cooling fan comes on. I would also recommend closing any other applications such as Word, Chrom etc. you have running. Occasionally RStudio will crash when running a deep learning model, so **save all your R scripts** before running the next commands.

```{r train model, eval=FALSE}
# Train the model with fit_generator
history <- model %>% fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # print progress
  verbose = 2
)
```

### Assessing the accuracy and loss
The results of the model training are stored in the `history` object, which can be plotted separately (note that your graph and results will differ):

```{r, eval=FALSE}
plot(history)

```
```{r, echo=FALSE}
cnnplot <- readRDS("historyplot.RDS")
plot(cnnplot)
```

For me there is evidence of overtraining after about 8 epochs, in that the training loss declines, but the validation loss flattens out, or even increases slightly. The accuracy graph shows the revere pattern, in that the training accuracy increases to over 90%, but the validation accuracy is stuck at around 55 to 60%. Whilst 60% is fairly low, keep in mind that:

* you are working on a very small dataset, with only 400 photos per species to build the model
* the quality of the images is quite variable. Some of the photos I have used include butterfly eggs, larvae or pupae, rather than adults.

## Saving your model for future use
It is useful to save your model so that you can re-use it later, without having to go through the whole process of re-training it. For something small, like this example, you can simple save the R data space with the `save.image` command. For larger models, especially if you are fine-tuning them and want to compare outputs and predictions, it is better to use the dedicated Keras `save_model_hdf5` which stores it in a special hdf5 format. You can retrieve a model using the `load_model_hdf5` command.

```{r plot and save results, eval=FALSE}
# The imager package also has a save.image function, so unload it to
# avoid any confusion
detach("package:imager", unload = TRUE)

# The save.image function saves your whole R workspace
save.image("animals.RData")

# Saves only the model, with all its weights and configuration, in a special
# hdf5 file on its own. You can use load_model_hdf5 to get it back.
#model %>% save_model_hdf5("animals_simple.hdf5")
```

## Testing your model
We can now test how good the model is with a completely independent dataset, that hasn't been seen by your model. In other words, the 100 photos per species in the `test` folder. We create an image_data_generator as before, and then "flow" all the images through. The one change is that we have set `shuffle = FALSE`. By default during training Keras shuffles the images to improve accuracy. However, you don't want to do this when comparing observed and predicted. We can then push the model into the `evaluate_generator()` function to display the loss and accuracy. These will always be **worse** than the ones during model training+validation, simply because the data are totally independent.

```{r, eval=FALSE}
path_test <- "test"

test_data_gen <- image_data_generator(rescale = 1/255)

test_images <- flow_images_from_directory(path_test,
   test_data_gen,
   target_size = target_size,
   class_mode = "categorical",
   classes = animal_list,
   shuffle = FALSE, # do not shuffle the images around
   batch_size = 1,  # Only 1 image at a time
   seed = 123)

# Takes about 3 minutes to run through all the images
model %>% evaluate_generator(test_images, 
                     steps = test_images$n)
```

For my data, I ended up with an accuracy of 58.7% which is actually only marginally poorer than that for the training+validation dataset.

## A word of warning about unbalanced data (optional)
Sometimes you may have data where the numbers of species are very unequal, e.g 500 images for a common species, but only 75 for a rare one. Such unbalanced data can skew accuracy measures, making them appear much better than they really are. Another scenario is camera trap data, where 90% of the photos might be blank, and only 10% contain an animal. If you have a poor model, it is still likely to correctly predict a blank (simply because there are so many of them). What really matters is whether it can spot what is going on in the small number of true positives.

There are several methods of resolving this issue of misleading accuracy scores with unbalanced data. Whilst we don't have unbalanced data here, it's useful to look at one approach, which is to compare the observed vs predicted for each species. This can be displayed in a 'confusion matrix'; ideally all the images fall on the main diagonal.

First of all, lets make predictions for our `test` photographs, but store the results in a `data.frame`:

```{r, eval=FALSE}
predictions <- model %>% 
  predict_generator(
    generator = test_images,
    steps = test_images$n
  ) %>% as.data.frame
colnames(predictions) <- animal_list
```

Have a look at the contents of the `predictions` table. Each row represents one photograph, with the values being the probability of it belonging to each species, the sum of each row being 1.000. So, rows 1-100 are brimstone, 101-200 are holly blue and 201-300 orange tip. Scan through the table: which species has it done best on, and which worse?

The easiest way to confirm your suspicions is to create a confusion matrix. Simply locate the highest probability in each row, and assume that is the predicted class:

```{r}
# Create 3 x 3 table to store data
confusion <- data.frame(matrix(0, nrow=3, ncol=3), row.names=animal_list)
colnames(confusion) <- animal_list

obs_values <- factor(c(rep(animal_list[1],100),
                       rep(animal_list[2], 100),
                       rep(animal_list[3], 100)))
pred_values <- factor(colnames(predictions)[apply(predictions, 1, which.max)])

library(caret)
conf_mat <- confusionMatrix(data = pred_values, reference = obs_values)
conf_mat
```

The above produces a lot of output, but the `caret` package is quite useful in that it produces nearly all the relevant statistics that you need. You can see from the confusion matrix that the main problem is we heavily over-predict orange tip, especially for brimestone photographs. This might be because the latter are also relatively pale, and in some photos look light coloured. Overall, however the Kappa statistic is highly significant, showing a significant relationship between observed and predicted.

A couple of terms you may have heard on the news in relation to medical tests:

* Sensitivity is the proportion of positives that are correctly identified (true positive rate). So for me, whilst 94% of Orange Tips are correctly identified, only 31% and 51% of brimstone and holly blue are correct. 
* Specificity is the proportion of negatives that are correctly identified (true negative rate). Here there are very few errors for brimstone or holly blue. i.e. when it was not a brimstone or holly blue (a negative) it was rarely incorrectly classed as one. In contrast, this was a problem with orange tips.

The relevant Wikipedia page gives a nice summary of these statistics, from a medical perspective, but it applies to ecology <https://en.wikipedia.org/wiki/Sensitivity_and_specificity>

## Making a prediction for a single image (optional)
Finally, let's display a single image from our `test` dataset, and make a prediction for it. Remember the numbering of your images in the `test` dataset is 501-600.

```{r}
# Original image
test_image_plt <- imager::load.image("test/hollyblue/spp_508.jpg")
plot(test_image_plt)

# Need to import slightly differently resizing etc. for Keras
test_image <- image_load("test/hollyblue/spp_508.jpg",
                                  target_size = target_size)

test_image <- image_to_array(test_image)
test_image <- array_reshape(test_image, c(1, dim(test_image)))
test_image <- test_image/255

# Now make the prediction, and print out nicely
pred <- model %>% predict(test_image)
pred <- data.frame("Species" = animal_list, "Probability" = t(pred))
pred <- pred[order(pred$Probability, decreasing=T),][1:3,]
pred$Probability <- paste(round(100*pred$Probability,2),"%")
pred
```


