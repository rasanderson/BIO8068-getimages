---
title: "Using wildlife images from Citizen Science databases"
subtitle: "BIO8068 Data visualisation in Ecology"
output:
  html_document: default
  word_document:
    reference_docx: template.docx
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rinat)
library(sf)
library(keras)

source("download_images.R")
gb_ll <- readRDS("gb_simple.RDS")
```

# Introduction
You may have noticed from looking at some of the Citizen Science databases we have explored earlier that records often have photographs associated with them. Go back and look at the National Biodiversity Network (NBN Atlas), Global Biodiversity Information Facility (GBIF) and xeno-canto birdsong databases and you'll see that some, but not all, records have photographs. The quality of photographs is extremely variable, and it is usually the more recently submitted records that are likely to have photographs.

The international iNaturalist website <https://www.inaturalist.org/> and UK-based iRecord website <https://www.brc.ac.uk/irecord/> both provide access to high quality Citizen Science datasets. Both databases have mobile phone apps through which users can submit records, and as a result a high proportion of records include photographs. The UK iRecord site is more restricted in that it does not allow general downloading of images (although they can be viewed on the website).

# The "data bottleneck" in deep learning
You have already created a simple Convolutional Neural Network (CNN) to identify different species, and this required you to have lots of photographs that had already been labelled, that you could then put into your training or validation datasets to create the models. One problem is obtaining sufficient data for machine learning models that has already been labelled correctly. Often this process has to be done by hand, and is time consuming.

# Aims and objectives
In this workshop you'll learn how to quickly obtain images from the iNaturalist Citizen Science database that can then be used in deep learning models. It will give you an understanding of the problems of data quality that can arise, as well as better technical insights. We shall pick several contrasting butterfly species as an example to work with.

# Bulk download brimstone butterfly images
Begin by loading the `rinat` package with `library(rinat)` which provides useful utility functions such as `get_inat_obs` that you have already used to retrieve records. You should also load the `sf` package with `library(sf)` as iNaturalist is an international biodiversity website, and for the purposes of this practical exercise we are going to restrict our search to Great Britain. Actually downloading the images takes a bit of R coding, and requires some additional packages to be installed. I have prepared a script, which will setup the additional packages, in particular `RCurl` for you, and also create a function called `download_images()` which does the hard work of downloading and renaming images. Finally, we shall define a bounding box for Great Britain based on the `gb_simple.RDS` file you have already used.

```{r, eval=FALSE}
library(rinat)
library(sf)

# Both download_images.R and gb_simple.RDS available on Canvas
source("download_images.R") 
gb_ll <- readRDS("gb_simple.RDS")
```

Once you have setup your working environment, we can search for records with the `get_inat_obs()` function. Look at the help for this function and you will see that it is quite flexible, in that you can specify a particular month or year. We will restrict our search to a maximum of 1000 records, and specify they must be of "research" quality.

We'll begin by searching for records of the brimstone butterfly, _Gonepteryx rhamni_ <https://butterfly-conservation.org/butterflies/brimstone> You could do a general searh for "brimstone" but this would also return records for the completely different brimstone moth _Opisthograptis luteolata_. They are both yellow in colour, but common names can be misleading, so stick to the Latin names:

```{r, eval=FALSE}
brimstone_recs <-  get_inat_obs(taxon_name  = "Gonepteryx rhamni",
                               bounds = gb_ll,
                               quality = "research",
                               # month=6,   # Month can be set.
                               # year=2018, # Year can be set.
                               maxresults = 500)
```

If you look at the records, you'll see that most of them have a URL to the actual image. If you copy a URL into your web browser you can have a look at one of them. If you wanted, you could `filter` your results, remember to `library(dplyr)`, at this stage for example to remove any records tagged as larvae rather than adults. Unfortunately, in general this sort of information is missing for most records, so is hard to automate. However, in the UK the larvae are most active May-July, so you could always `filter` out records for those months if you find too many caterpillar photographs.

Now use the `download_images()` function to retrieve the images. This has the following arguments:

* `spp_recs` The list of records from iNaturalist, possibly after a `filter` to improve quality (required)
* `spp_folder` The name of the folder into which the image files will be saved (required)
* `image_folder` By default this is a subfolder within your RStudio Project called `images` and will be created for you if it does not exist (optional)

Depending on the speed of your internet connection it may take 30 minutes or more to download a large number of images. A popup screen will indicate progress, so you will have an understanding of how long it is likely to take:

```{r, eval=FALSE}
download_images(spp_recs = brimstone_recs, spp_folder = "brimstone")
```

As the files download, you'll notice that they all have exactly the same name of `medium.jpg` so the `download_images()` function renames them `spp_1.jpg`, `spp_2.jpg`, `spp_3.jpg` etc. to avoid over-writing. They will be stored in a subfolder called `brimstone` within your `images` folder. Each individual file is fairly small, but collectively your downloaded images will probably be over 50 Mb. Go to the `brimstone` subfolder in File Explorer (Windows) or Finder (Mac) and look at some of them. Most of mine are of adult butterflies, but there are a small number of images with larvae, or the butterfly is difficult to see. There are also quite a lot of images where the main object in the photograph is actually a flower from which the butterflies are collecting nectar, typically purple or pink-coloured flowers. Be alert that if you train a model on other species of butterflies that have similar feeding preferences the model may learn to identify the flowers, rather than the butterflies!

# Bulk download holly blue and orange tip butterfly records and images
Now we'll repeat the process for two other species of common butterflies in a similar way, the holly blue <https://butterfly-conservation.org/butterflies/holly-blue> and the orange tip <https://butterfly-conservation.org/butterflies/orange-tip>:

```{r, eval=FALSE}
# Holly blue; Celastrina argiolus
hollyblue_recs <-  get_inat_obs(taxon_name  = "Celastrina argiolus",
                               bounds = gb_ll,
                               quality = "research",
                               maxresults = 500)


# Orange tip; Anthocharis cardamines
orangetip_recs <-  get_inat_obs(taxon_name  = "Anthocharis cardamines",
                               bounds = gb_ll,
                               quality = "research",
                               maxresults = 500)
```

One thing to be alert to with both these species is that the males are much more brightly coloured than the females. The overwhelming majority of records of butterflies on iNaturalist do not unfortunately distinguish between male and female photographs, and this will decrease the accuracy of any deep learning models. After you have downloaded the records, again explore them using `View(hollyblue_recs)` or `View(orangetip_recs)`. Nearly all the records include a link to a photograph.

Next download the images, which will be stored in two subfolders, named accordingly, within your `images` folder. You may want to make a cup of tea or coffee whilst the downloads take place...

```{r, eval=FALSE}
download_images(spp_recs = hollyblue_recs, spp_folder = "hollyblue")
download_images(spp_recs = orangetip_recs, spp_folder = "orangetip")
```

If you look at the photographs in the `hollyblue` and `orangetip` subfolders, you'll notice that the imagery for the latter is not as well-defined. This is likely to give poorer performance in any deep-learning model.

## Split into training and validation sets
Ideally we would have several thousand images for each of our three species, indeed many deep learning approaches uses tens of thousands of images. However I did not want you to have to wait too long to download the images, or do the training, but keep this weakness in mind when you analyse the data. We will split the images into 400 for training, and 100 for validation. You could create the appropriate folder tree in File Explorer or Finder, but it is easier to do in R. We will copy the images into a folder called `train_valid` and split them accordingly.

```{r, echo=FALSE, eval=FALSE}
# Create the 6 subfolders within the train_valid tree
dir.create("train_valid/train/brimstone", recursive=TRUE, showWarnings=FALSE)
dir.create("train_valid/train/hollyblue", recursive=TRUE, showWarnings=FALSE)
dir.create("train_valid/train/orangetip", recursive=TRUE, showWarnings=FALSE)
dir.create("train_valid/valid/brimstone", recursive=TRUE, showWarnings=FALSE)
dir.create("train_valid/valid/hollyblue", recursive=TRUE, showWarnings=FALSE)
dir.create("train_valid/valid/orangetip", recursive=TRUE, showWarnings=FALSE)
```

```{r, eval=FALSE}
# Create the 6 subfolders within the train_valid tree
dir.create("train_valid/train/brimstone", recursive=TRUE)
dir.create("train_valid/train/hollyblue", recursive=TRUE)
dir.create("train_valid/train/orangetip", recursive=TRUE)
dir.create("train_valid/valid/brimstone", recursive=TRUE)
dir.create("train_valid/valid/hollyblue", recursive=TRUE)
dir.create("train_valid/valid/orangetip", recursive=TRUE)
```

Note that to do a deep learning model strictly correct we would have such a large dataset that we could split it into 3 datasets, "training", "validation" and "testing". The test dataset is not used at all as the model is built and validated, and simply for an assessment of the reliability of the final predictions. As we have 1500 files to process, it will take 2 to 3 minutes to copy them.

```{r, eval=FALSE}
# Now copy 400 images per spp into train, and 100 into valid
set.seed(123)
train_nos <- sort(sample(1:500, 400))
valid_nos <- (1:500)[1:500 %in% train_nos == FALSE]

# Copy over the random sets of 400 training images
# subfolder name is "train", and uses "train_nos" for index
for(image_file in 1:400){
  file_src_brimstone  <- paste0("images/brimstone/spp_", train_nos[image_file], ".jpg")
  file_dest_brimstone <- paste0("train_valid/train/brimstone/spp_", train_nos[image_file], ".jpg")
  file_src_hollyblue  <- paste0("images/hollyblue/spp_", train_nos[image_file], ".jpg")
  file_dest_hollyblue <- paste0("train_valid/train/hollyblue/spp_", train_nos[image_file], ".jpg")
  file_src_orangetip  <- paste0("images/orangetip/spp_", train_nos[image_file], ".jpg")
  file_dest_orangetip <- paste0("train_valid/train/orangetip/spp_", train_nos[image_file], ".jpg")
  file.copy(file_src_brimstone, file_dest_brimstone)
  file.copy(file_src_hollyblue, file_dest_hollyblue)
  file.copy(file_src_orangetip, file_dest_orangetip)
}

# Copy over the random sets of 100 validation images
# subfolder name is "valid" and now uses "valid_nos" as index
for(image_file in 1:100){
  file_src_brimstone  <- paste0("images/brimstone/spp_", valid_nos[image_file], ".jpg")
  file_dest_brimstone <- paste0("train_valid/valid/brimstone/spp_", valid_nos[image_file], ".jpg")
  file_src_hollyblue  <- paste0("images/hollyblue/spp_", valid_nos[image_file], ".jpg")
  file_dest_hollyblue <- paste0("train_valid/valid/hollyblue/spp_", valid_nos[image_file], ".jpg")
  file_src_orangetip  <- paste0("images/orangetip/spp_", valid_nos[image_file], ".jpg")
  file_dest_orangetip <- paste0("train_valid/valid/orangetip/spp_", valid_nos[image_file], ".jpg")
  file.copy(file_src_brimstone, file_dest_brimstone)
  file.copy(file_src_hollyblue, file_dest_hollyblue)
  file.copy(file_src_orangetip, file_dest_orangetip)
}
```

## Train up your deep learning model
Now you have downloaded your image data from iNaturalist and sorted it at random into training and validation sets, you can create a deep learning model using a convolutional neural network, similar to your earlier example. The following code is almost identical to the one you used earlier, with only some minor changes. Remember to load the `keras` package using `library(keras)` before you continue:

```{r}
# list of animals to model; these names must match folder names
animal_list <- c("brimstone", "hollyblue", "orangetip")

# number of output classes (i.e. fruits)
output_n <- length(animal_list)

# image size to scale down to (original images vary but about 400 x 500 px)
img_width <- 150
img_height <- 150
target_size <- c(img_width, img_height)

# Full-colour Red Green Blue = 3 channels
channels <- 3

# path to image folders
train_image_files_path <- "train_valid\\train\\"
valid_image_files_path <- "train_valid\\valid\\"
```

Next rescale your images to a smaller size and import them

```{r}
# Rescale from 255 to between zero and 1
train_data_gen = image_data_generator(
  rescale = 1/255
)

valid_data_gen <- image_data_generator(
  rescale = 1/255
)

# training images
train_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                                    train_data_gen,
                                                    target_size = target_size,
                                                    class_mode = "categorical",
                                                    classes = animal_list,
                                                    seed = 42)

# validation images
valid_image_array_gen <- flow_images_from_directory(valid_image_files_path, 
                                                    valid_data_gen,
                                                    target_size = target_size,
                                                    class_mode = "categorical",
                                                    classes = animal_list,
                                                    seed = 42)

```

Next, check that we seem to have the right number off classes and images:

```{r check generator}
# Check that things seem to have been read in OK
cat("Number of images per class:")
table(factor(train_image_array_gen$classes))
cat("Class labels vs index mapping")
train_image_array_gen$class_indices
```

```{r, echo=FALSE}
detach("package:keras", unload = TRUE)
detach("package:imager", unload = TRUE)
```

Next define some 'hyper-parameters' such as batch size for numbers of images flowing through the system with each 'epoch'

```{r final setup, eval=FALSE}
# number of training samples
train_samples <- train_image_array_gen$n
# number of validation samples
valid_samples <- valid_image_array_gen$n

# define batch size and number of epochs
batch_size <- 32 # Typical default, though possibly a little high given small dataset
epochs <- 10
```

Now you define how your CNN is structured.

```{r define CNN structure, eval=FALSE}
# initialise model
model <- keras_model_sequential()

# add layers
model %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), input_shape = c(img_width, img_height, channels), activation = "relu") %>%

  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), activation = "relu") %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %>%
  layer_dense(100, activation = "relu") %>%
  layer_dropout(0.5) %>%
  
  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n, activation = "softmax") 
```


Remember to check the CNN structure before compiling and running it

```{r check CNN structure, eval=FALSE}
print(model)
```

Define the error terms and accuracy measures. Use `categorical_crossentropy` as we have more than two species:

```{r compile, eval=FALSE}
# Compile the model
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),
  metrics = "accuracy"
)
```
Finally, train up the model. **This is slow**. Depending on your PC, it may take more than 30 minutes to complete. On my PC it takes about 10 minutes, but runs at 100% CPU and the cooling fan comes on. I would also recommend you save all your R script before running the next code, and closing any other applications you have running. Occasionally RStudio will crash when running a deep learning model, so you need to have everything saved, and free up as much PC memory as possible.

```{r train model, eval=FALSE}
# Train the model with fit_generator
history <- model %>% fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # print progress
  verbose = 2
)
```

The results of the model training are stored in the `history` object, which can be plotted, and it is useful to save your model. For something small, like this example, you can simple save the R data space with the `save.image` command. For larger models, especially if you are fine-tuning them and want to compare outputs and predictions, it is better to use the dedicated Keras `save_model_hdf5` which stores it in a special hdf5 format. You can retrieve a model using the `load_model_hdf5` command.

```{r plot and save results, eval=FALSE}
plot(history)
detach("package:imager", unload = TRUE)
save.image("animals.RData")
#model %>% save_model_hdf5("animals_simple.hdf5")
```

```{r, echo=FALSE}
cnnplot <- readRDS("historyplot.RDS")
plot(cnnplot)
```

**Note** : Your results will probably be different from mine, as you will have a different random number generator, and possibly different images from iNaturalist. There is evidence of over-training after about 6 or 7 "epochs" of training the model, so we really want to look at the results there. Even though it is based on a very small dataset, and the images are of variable quality (quite poor for many of the brimstone butterfly photos), we are nevertheless managing to hit almost 70% accuracy.

## Optional - taking the analyses further
Feel free to re-run the deep learning model with augmentation to try and improve the accuracy (see example from previous practical: the R code should function without needing major changes). You can also go into the relevant subfolders for the different butterflies and manually delete any images that seem unclear or of poor quality. If you, as a human being, don't think a photo is very good, then any deep learning model will struggle much more.